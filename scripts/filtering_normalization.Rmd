---
title: "filtering_normalization"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(reshape2)
library(phyloseq)
library(rstatix)
library(pbapply)
library(metagenomeSeq)
library(DESeq2)
```
# Read raw data
```{r}
ps_not_norm_comp <- readRDS("../data/ps_not_norm_age_filter_complete_family.rds")
output_data = "../results/"
dir.create(paste0(output_data, 'Normalized/'))

sup_file_df<-as.data.frame(ps_not_norm_comp@sam_data)

filt_sup_file_df<-sup_file_df[,colnames(ps_not_norm_comp@sam_data)[-which(colnames(ps_not_norm_comp@sam_data) %in% c("phenotype", "Sequencing.run.ID..OmicsFileSet._collapsed", "Biospecimen.Barcode", "SG.Project.ID", "Host.ID", "Host.Name", "Biospecimen.ID" , "Biospecimen.Name", "Biospecimen.Description" , "Biospecimen.mass", "Biospecimen.mass.unit" , "Biospecimen.Date.Collected"  , "NCBI.taxID" , "Organism", "Within.study.sampling.date", "Disease.focus" , "Natural.sibling.included.in.present.study" , "Biospecimen.type"  , "Specimen.preservation.method"  , "Specimen.status.upon.receipt", "LR6.prediction..M3." , "LR6.probability.not.ASD..M3.", "LR6.probability.ASD..M3.", "LR10.prediction..M3." , "LR10.probability.not.ASD..M3." ,  "LR10.probability.ASD..M3.", "LR5.prediction..M3."   , "LR5.probability.not.ASD..M3.", "LR5.probability.ASD..M3."  , "Age..years."  , "Other.stool.sample.collection.method.explained..M3.", "Time.of.sampling"  , "Stool.sample.challenges.explained..M3.", "Autism.spectrum.disorder", "Family.group.ID")) ] ]   

write.csv(filt_sup_file_df,  "../results/Supplementary_File_7.csv")

```

# Normalization
```{r normalization}


#Filtering of the prevalence: 
###Declare function to filter
filterTaxaByPrevolence <- function(ps, percentSamplesPresentIn){
  prevalenceThreshold <- percentSamplesPresentIn * nsamples(ps)
  toKeep <- apply(data.frame(otu_table(ps)), 1, function(taxa) return(sum(taxa > 0) > prevalenceThreshold))
  ps_filt <- prune_taxa(toKeep, ps)
  return(ps_filt)
}

#CSS norm function
#We actually will plot everything with CSS 
CSS_norm<-function(ps){
  ps.metaG<-phyloseq_to_metagenomeSeq(ps)
  p_stat = cumNormStatFast(ps.metaG)
  ps.metaG = cumNorm(ps.metaG, p = p_stat)
  ps.metaG.norm <- MRcounts(ps.metaG, norm = T)
  ps_CSS<-phyloseq(otu_table(ps.metaG.norm, taxa_are_rows = T), sample_data(ps),tax_table(ps))
  return(ps_CSS)
}

#Deseq norm 
deSeqNorm <- function(ps){
  ps_dds <- phyloseq_to_deseq2(ps, ~ phenotype + Family.group.ID)
  ps_dds <- estimateSizeFactors(ps_dds, type = "poscounts")
  ps_dds <- estimateDispersions(ps_dds)
  abund <- getVarianceStabilizedData(ps_dds)
  abund <- abund + abs(min(abund)) #don't allow deseq to return negative counts
  ps_deSeq <- phyloseq(otu_table(abund, taxa_are_rows = T), sample_data(ps), tax_table(ps))
  return(ps_deSeq)
}


#fixing the mapping file for stats by adding categorical vs non catergorical
metadata_ok<-sample_data(ps_not_norm_comp)
write.csv(metadata_ok, "../data/sam_data.csv")
map <- read.csv("../data/sam_data.csv", row.names = 1)
meta_cat <- read.csv("../data/updated_metacategories.csv", row.names = 1)
meta_cat = meta_cat[rownames(meta_cat) %in% colnames(map), ]

for (i in rownames(meta_cat)[meta_cat$permanova != FALSE]){
  print(i)
 if (meta_cat[i, ]$permanova == "Categorical") {
    map[,i] <- as.factor(map[,i])
 } else {
    map[,i] <- as.numeric(map[,i])
 }
}


makeFieldsNumeric <- function(map){
  handleNAs <- function(vec){
    vec[vec == ""] <- "NA"
    vec[is.na(vec)] <- "NA"
    return(vec)
  }
  
  map$Stool.frequency <- handleNAs(as.character(map$Stool.frequency))
  map$Stool.frequency[as.character(map$Stool.frequency) == "Less than 1"] = 0
  map$Stool.frequency[as.character(map$Stool.frequency) == "5 or more"] = 5
  map$Dairy..consumption.frequency...longitudinal.[map$Dairy..consumption.frequency...longitudinal. == 5] <- "3-4 meals per week"
  #map$LR2[map$LR2 == "1 (ASD)"] = 1
  #map$LR2[map$LR2 == "0 (non-ASD"] = 0
  
  
  freq_dict_2 <- list("Never" = "0", "Rarely" = "1", "Occasionally" = "2", "Regularly" = "3", "Weekly" = "4", "weekly" = "4",
                      "Several time weekly" = "5", "Several times weekly" = "5", "Daily" = "6", "NA" = "NA")
  dict_2_items <- c("Whole.grain..consumption.frequency.", "Fermented.vegetable..consumption.frequency.", "Dairy..consumption.frequency.","Meat..consumption.frequency." , "Fruit..consumption.frequency.", "Meals.prepared.at.home..consumption.frequency.",   "Ready.to.eat.meals..consumption.frequency.", "Red.meat..consumption.frequency.", "Olive.oil.used.in.cooking..M3.", "Seafood..consumption.frequency.",   "Sweetened.drink..consumption.frequency.", "Vegetable..consumption.frequency.",
                    "Restaurant.prepared.meals..consumption.frequency.", "Sugary.food..consumption.frequency.", "Probiotic..consumption.frequency.", "Vitamin.B.complex.supplement..consumption.frequency.", "Vitamin.D..consumption.frequency.")
  for(item in dict_2_items){
    print(item)
    #tmp <- rep(NA, nrow(map))
    #freqs <- handleNAs(map[,item])
    #numeric_rep <- unlist(freq_dict_2[freqs])
    tmp <- map
    tmp[,item] <- as.character(tmp[,item])
    for (n in names(freq_dict_2)) {
    tmp[,item][which(tmp[,item] == n )] <- rep(freq_dict_2[[n]], length(tmp[,item][which(tmp[,item] == n )]))
    }
    map[,item] <- as.numeric(tmp[,item])
    #numeric_rep <-as.numeric(numeric_rep)
    #print(paste("Numeric rep length: ", length(numeric_rep)))
    #print(sum(!is.na(freqs)))
    #tmp[!is.na(freqs)] <- as.numeric(numeric_rep)  
    #map[ , item] <- tmp
  }
  
  freq_dict_1 <- list("Never or less than once per week" = "0", "3-4 meals per week" = "1", "5" = "2", "7-10 meals per week" = "3", "Almost every meal" = "4", "NA" = "NA")
  dict_1_items <- c("Starchy.food..consumption.frequency...longitudinal.", "Meats.and.seafood..consumption.frequency...longitudinal.", "Bread..consumption.frequency...longitudinal.", "Dairy..consumption.frequency...longitudinal.", "Dietary.fat.and.oil..consumption.frequency...longitudinal.", "Vegetable..consumption.frequency...longitudinal.", 
                    "Fruit..consumption.frequency...longitudinal.")
  for(item in dict_1_items){
    print(item)
    #tmp <- rep(NA, nrow(map))
    #freqs <- handleNAs(map[ , item])
    #numeric_rep <- unlist(freq_dict_1[freqs])
    #numeric_rep <- as.numeric(numeric_rep)
    #print(paste("Numeric rep length: ", length(numeric_rep)))
    #print(sum(!is.na(freqs)))
    #tmp[!is.na(freqs)] <- as.numeric(numeric_rep)  
    #map[ , item] <- tmp
    
    tmp <- map
    tmp[,item] <- as.character(tmp[,item])
    for (n in names(freq_dict_1)) {
    tmp[,item][which(tmp[,item] == n )] <- rep(freq_dict_1[[n]], length(tmp[,item][which(tmp[,item] == n )]))
    }
    map[,item] <- as.numeric(tmp[,item])
  }
  
  #may add more, but these variable only apply to phenotype for autism
    freq_dict_2 <- list("Able to speak fluently" = 4,"Phrase speech"=3, "Single word speech"=2, "Little to no speech" = 1,"Able to have conversation" = 4, "Limited conversation ability" = 3, "Difficulty with conversation" = 2, "Cannot have a conversation" = 1, "Understands about half of words" = 1, "Understands few or no words"= 0, "Understands many words" = 2,  "Understands most words"= 3, "Understands nearly all words" = 4 , "Never" = 1, "Rarely" = 2, "Sometimes" = 3 , "Regularly" = 4, "No opportunity to play with other children" = NA, "Consistent eye contact" = 4,  "Little or no eye contact" = 1, "Some eye contact" = 3, "Uncertain about behavioral development" = NA, "Developmentally delayed (autism)" = 1, "Developmentally delayed (not clearly autism)" = 2, "Some developmental delays" = 3, "Met all developmental milestones" = 4 , "Does not imitate others" =1, "Imitates others when prompted" = 2, "Imitates actions or gestures of others" = 4 ,"NA" = 4)
  dict_2_items <- c("Language.ability.and.use", "Conversation.ability", "Understands.speech", "Plays.imaginatively.when.alone", "Plays.imaginatively.with.others",  "Plays.in.a.group.with.others", "Eye.contact.finding", "Childhood.behavioral.development.finding", "Picks.up.objects.to.show.to.others", "Imitation.behavior")
  for(item in dict_2_items){
    print(item)
    #tmp <- rep(NA, nrow(map))
    #freqs <- handleNAs(map[ , item])
    #numeric_rep <- unlist(freq_dict_2[freqs])
    #print(paste("Numeric rep length: ", length(numeric_rep)))
    #print(sum(!is.na(freqs)))
    #tmp[!is.na(freqs)] <- as.numeric(numeric_rep)  
    #tmp[is.na(tmp)] <- 4
    #map[ , item] <- tmp
    
    tmp <- map
    tmp[,item] <- as.character(tmp[,item])
    for (n in names(freq_dict_2)) {
    tmp[,item][which(tmp[,item] == n )] <- rep(freq_dict_2[[n]], length(tmp[,item][which(tmp[,item] == n )]))
    }
    map[,item] <- as.numeric(tmp[,item])
  }
  
   #may add more, but these variable only apply to phenotype for autism
    freq_dict_3 <- list( "Never" = 1, "Sometimes" = 2, "Regularly" = 3,  "NA" = 1, "Constant sleep difficulties" = 3,"Some sleep difficulties" = 2, "Healthy sleep pattern" = 1, "Highly sensitive to typical sounds" = 3,  "Sensitive to typical sounds" = 2, "Not bothered by typical sounds" = 1, "No self-injurious behavior" = 1,"Mild self-harming behavior" = 2, "Dangerous or frequent self-harming behavior" = 3,  "No issues" = 1, "Continuous" = 3, "No elevated anxiety" = 1, "Somewhat elevated anxiety" = 2, "Elevated anxiety" = 3)
  dict_3_items <- c("Repetitive.motion", "Sleep.pattern.finding", "Response.to.typical.sounds", "Self.injurious.behavior.finding", "Gastrointestinal.problems..M3.", "Recent.anxiety..caretaker.reported.")
  for(item in dict_3_items){
    print(item)
    #tmp <- rep(NA, nrow(map))
    #freqs <- handleNAs(map[ , item])
    #numeric_rep <- unlist(freq_dict_3[freqs])
    #print(paste("Numeric rep length: ", length(numeric_rep)))
    #print(sum(!is.na(freqs)))
    #tmp[!is.na(freqs)] <- as.numeric(numeric_rep)
    #tmp[is.na(tmp)] <- 1
    #map[ , item] <- tmp
    
    tmp <- map
    tmp[,item] <- as.character(tmp[,item])
    for (n in names(freq_dict_3)) {
    tmp[,item][which(tmp[,item] == n )] <- rep(freq_dict_3[[n]], length(tmp[,item][which(tmp[,item] == n )]))
    }
    map[,item] <- as.numeric(tmp[,item])
  }
  
  
  map <- map[!duplicated(map$Biospecimen.Barcode), ]
  rownames(map) <- map$Biospecimen.Barcode
  map$Stool.frequency <- as.numeric(map$Stool.frequency)
  return(map)

}

dict_1_items <- c("Starchy.food..consumption.frequency...longitudinal.", "Meats.and.seafood..consumption.frequency...longitudinal.", "Bread..consumption.frequency...longitudinal.", "Dairy..consumption.frequency...longitudinal.", "Dietary.fat.and.oil..consumption.frequency...longitudinal.", "Vegetable..consumption.frequency...longitudinal.", 
                    "Fruit..consumption.frequency...longitudinal.")
dict_2_items <- c("Language.ability.and.use", "Conversation.ability", "Understands.speech")


map<-makeFieldsNumeric(map)


map_levels<-sapply(map, levels)
map_levelscount<-sapply(map_levels, length)
mapnotfac <- names(map_levelscount[which(map_levelscount >= 18)])

for (i in mapnotfac){
  map[,i]<-as.character(map[,i])
}

#Round years
map$Age..years. <-round(map$Age..years.)

#create scores for social, learning, and anxiety
social <- c("Language.ability.and.use", "Conversation.ability", "Plays.imaginatively.when.alone", "Plays.imaginatively.with.others", "Plays.in.a.group.with.others", "Eye.contact.finding", "Picks.up.objects.to.show.to.others", "Imitation.behavior")

learning <- c("Understands.speech", "Childhood.behavioral.development.finding")

anxiety <- c("Repetitive.motion", "Sleep.pattern.finding", "Response.to.typical.sounds", "Self.injurious.behavior.finding", "Recent.anxiety..caretaker.reported.")

map$social_score_sum<-rowSums(map[, social])
map$learning_score_sum<-rowSums(map[, learning])
map$anxiety_score_sum <-rowSums(map[, anxiety])

map$social_score_avg<-rowMeans(map[, social])
map$learning_score_avg<-rowMeans(map[, learning])
map$anxiety_score_avg <-rowMeans(map[, anxiety])



sample_data(ps_not_norm_comp) <- map

#Remove Breastfed and their families
breast_fed <- c("089_A","054_N", "158_N" )

b_fed<-unique(ps_not_norm_comp@sam_data$Family.group.ID[which(ps_not_norm_comp@sam_data$Host.Name %in% breast_fed)])

#remove the whole family
for (i in b_fed){
  ps_not_norm_comp<- prune_samples(ps_not_norm_comp@sam_data$Family.group.ID != i, ps_not_norm_comp)

}


#Remove Possible Contradictions and their families
possible_phen_contra <- c("020_A","131_A", "184_A" )

p_con<-unique(ps_not_norm_comp@sam_data$Family.group.ID[which(ps_not_norm_comp@sam_data$Host.Name %in% possible_phen_contra)])

#remove the whole family
for (i in p_con){
  ps_not_norm_comp<- prune_samples(ps_not_norm_comp@sam_data$Family.group.ID != i, ps_not_norm_comp)

}

#add season during collection
ps_not_norm_comp@sam_data$Season <- gsub("-01-", "Winter", ps_not_norm_comp@sam_data$Biospecimen.Date.Collected)
wint <- c("-02-", "-12-")
for (i in wint){
ps_not_norm_comp@sam_data$Season <- gsub(i, "Winter", ps_not_norm_comp@sam_data$Season)
}

spring <- c("-03-", "-04-", "-05-")
for (i in spring){
ps_not_norm_comp@sam_data$Season <- gsub(i, "Spring", ps_not_norm_comp@sam_data$Season)
}

summer <- c("-06-", "-07-", "-08-")
for (i in summer){
ps_not_norm_comp@sam_data$Season <- gsub(i, "Summer", ps_not_norm_comp@sam_data$Season)
}

fall<- c("-09-", "-10-", "-11-")
for (i in fall){
ps_not_norm_comp@sam_data$Season <- gsub(i, "Fall", ps_not_norm_comp@sam_data$Season)
}

ps_not_norm_comp@sam_data$Season <- gsub("2018","", ps_not_norm_comp@sam_data$Season)
ps_not_norm_comp@sam_data$Season <- gsub("2017","", ps_not_norm_comp@sam_data$Season)

for (i in c("1","2","3", "4", "5", "6", "7", "8", "9", "0")){
ps_not_norm_comp@sam_data$Season <- gsub(i, "", ps_not_norm_comp@sam_data$Season)
}
ps_not_norm_comp@sam_data$Season <- as.factor(ps_not_norm_comp@sam_data$Season)

ps_not_norm_comp@sam_data$Annual.household.income_rank <- gsub("150001-200000", 5, ps_not_norm_comp@sam_data$Annual.household.income)

ps_not_norm_comp@sam_data$Annual.household.income_rank <- gsub("More than 200000", 6, ps_not_norm_comp@sam_data$Annual.household.income_rank)

ps_not_norm_comp@sam_data$Annual.household.income_rank <- gsub("Less than 20000", 1, ps_not_norm_comp@sam_data$Annual.household.income_rank)

ps_not_norm_comp@sam_data$Annual.household.income_rank <- gsub("20001-40000", 2, ps_not_norm_comp@sam_data$Annual.household.income_rank)

ps_not_norm_comp@sam_data$Annual.household.income_rank <- gsub("40001-80000", 3, ps_not_norm_comp@sam_data$Annual.household.income_rank)

ps_not_norm_comp@sam_data$Annual.household.income_rank <- gsub("80001-150001", 4, ps_not_norm_comp@sam_data$Annual.household.income_rank)

ps_not_norm_comp@sam_data$Annual.household.income_rank <- as.numeric(ps_not_norm_comp@sam_data$Annual.household.income_rank)

#Now we remove the taxa present in less than 3 % of the samples with some basic filtering 
filtered_ps003<-filterTaxaByPrevolence(ps_not_norm_comp, 0.03)
filtered_ps003
saveRDS(filtered_ps003, file=paste0(output_data, "Normalized/ps_not_norm_comp_pass_min_postDD_min0.03.rds"))

# CSS normalization
ps_css<- CSS_norm(filtered_ps003)
saveRDS(ps_css, file=paste0(output_data, "Normalized/ps_CSS_pass_min_postDD_min0.03.rds"))

# DESeq normalization
ps_deseq <- deSeqNorm(filtered_ps003)
saveRDS(ps_deseq, file=paste0(output_data, "Normalized/ps_DeSeq_pass_min_postDD_min0.03.rds"))

# TSS normalization
propDF=prop.table(as.matrix(otu_table(filtered_ps003)), margin=2)
ps_tss <- phyloseq(otu_table(propDF, taxa_are_rows=TRUE), 
                                               tax_table(filtered_ps003), 
                                               sample_data(filtered_ps003))
saveRDS(ps_tss, paste0(output_data, "Normalized/ps_tss_pass_min_postDD_min0.03.rds"))


#Extra normalization to justify removal of breastfed
ps_raw <- readRDS("../data/ps_not_norm_age_filter_complete_family.rds")
#Remove Possible Contradictions and their families
possible_phen_contra <- c("020_A","131_A", "184_A" )

ps_con<-unique(ps_raw@sam_data$Family.group.ID[which(ps_raw@sam_data$Host.Name %in% possible_phen_contra)])


#remove the whole family
for (i in p_con){
  ps_raw<- prune_samples(ps_raw@sam_data$Family.group.ID != i, ps_raw)

}
ps_raw <- filterTaxaByPrevolence(ps_raw, 0.03)
ps_deseq_raw <- deSeqNorm(ps_raw)
saveRDS(ps_deseq_raw, file=paste0(output_data, "Normalized/ps_DeSeq_raw.rds"))


```

### Friedman test to remove taxa that are too inconsistent between timepoints
```{r}
ps_deseq <- readRDS(paste0(output_data, "Normalized/ps_DeSeq_pass_min_postDD_min0.03.rds"))
ps_css <- readRDS(paste0(output_data, "Normalized/ps_CSS_pass_min_postDD_min0.03.rds"))
ps_no_norm <- readRDS(paste0(output_data, "Normalized/ps_not_norm_comp_pass_min_postDD_min0.03.rds"))


getASVsChangeSigOverTime <- function(ps){
  asv_table<-t(otu_table(ps))
  asv_table <- as.data.frame(asv_table)
  asv_table$timepoint <- ps@sam_data$Within.study.sampling.date
  asv_table$timepoint <- gsub(" ", "", asv_table$timepoint)
  asv_table$timepoint <- as.factor(asv_table$timepoint)
  asv_table$Host.Name <- ps@sam_data$Host.Name
  
  asv_long <- melt(asv_table, id = c("timepoint", "Host.Name"))
  change_over_timepoints_pvals <- pbsapply( unique(asv_long$variable), function(seq){
    asv_tmp <- asv_long[asv_long$variable == seq, ]
    asv_tmp <- select(asv_tmp, -c("variable"))
    res.fried <- asv_tmp %>% friedman_test(value ~ timepoint |Host.Name)
    return(res.fried$p)
  })
  asvs <- unique(asv_long$variable)[change_over_timepoints_pvals < .1]
  return(asvs)
}

#Set threshold for p value at .1 - If taxa are changing that much over 3 time points, their fluctuations are likely due to seasons or dietary
#changes rather than long standing gut-brain axis phenomenon
asv_deseq <- as.character(getASVsChangeSigOverTime(ps_deseq))
asv_css <- as.character(getASVsChangeSigOverTime(ps_css))
asv_allsamps <- as.character(getASVsChangeSigOverTime(ps_deseq_raw))

asv_intersect <- intersect(asv_deseq, asv_css)

#remove these taxa from phyloseqs (since they are noise essentially)
keep = taxa_names(ps_deseq)[!(taxa_names(ps_deseq) %in% asv_deseq)]
ps_deseq_filt <- prune_taxa(keep, ps_deseq)

keep = taxa_names(ps_css)[!(taxa_names(ps_css) %in% asv_css)]
ps_css_filt <- prune_taxa(keep, ps_css)

keep = taxa_names(ps_no_norm)[!(taxa_names(ps_no_norm) %in% asv_intersect)]
ps_no_norm_filt <- prune_taxa(keep, ps_no_norm)

keep = taxa_names(ps_deseq_raw)[!(taxa_names(ps_deseq_raw) %in% asv_allsamps)]
ps_raw_all_samp_filt <- prune_taxa(keep, ps_deseq_raw)


dir.create(path = paste0(output_data, "Filtered"))
saveRDS(ps_deseq_filt, paste0(output_data, "Filtered/ps_deseq_friedfilt.rds"))
saveRDS(ps_css_filt, paste0(output_data, "Filtered/ps_css_friedfilt.rds"))
saveRDS(ps_no_norm_filt, paste0(output_data, "Filtered/ps_no_norm_friedfilt.rds"))
saveRDS(ps_raw_all_samp_filt, paste0(output_data, "Filtered/ps_allsamps_deseq_norm_friedfilt.rds"))
```