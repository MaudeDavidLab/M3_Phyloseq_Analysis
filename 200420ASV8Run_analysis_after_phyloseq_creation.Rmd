---
title: "Longitudinal analysis"
author: "Updated and Added Upon by Austin Martin (Previous versions made by Maude David and Shoko Iwai)"
date: "`r Sys.Date()`"
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(substr(inputFile,1,nchar(inputFile)-4),'_',Sys.Date(),'.html')) })
output:
  html_document:
    number_sections: true
    toc: true
    toc_float: true
---

# Setup


## Visualization of significant taxa
```{r visualization_fun}
### functions to plot
make_vis_plots <- function(ps_norm, grouping, tax, plot_type=c('box', 'bar')){
  # ps should be a normalized (DESeq or CSS) phyloseq object
  # grouping should match the column name in the sample_data
  # tax is a taxonomical bin id (ASV) in the counts table to plot
  
  # subset phyloseq object to select ASV of interest
  ps_filt=prune_taxa(taxa_names(ps_norm) %in% tax, ps_norm)
  
  # get normalized counts
  plot_table<-data.table(otu_table(ps_filt), keep.rownames=TRUE)[rn %in% tax]
  # add very small value, min/100000 to 0
  plot_table <- melt(plot_table, id.vars='rn')
  plot_table$value <- plot_table$value+min(plot_table[value!=0]$value)/100000
  
  # add metadata
  groupDT=data.table(data.frame(sample_data(ps_filt)[, c(grouping, 'Within.study.sampling.date')]), keep.rownames=TRUE)
  setnames(groupDT, 'rn', 'variable')
  plot_table <- merge(plot_table, groupDT, by='variable', all.x=TRUE)
  
  # change variable to general name
  setnames(plot_table, grouping, 'Group')

  # boxplot
  if(plot_type=='box'){
    ggplot(data=plot_table, aes(x=Within.study.sampling.date, y = value, fill=Group)) + 
      geom_boxplot(outlier.color=NA) +
      geom_jitter(position=position_jitterdodge(0.2), cex=1.5, color="gray44") + 
      labs(title =deparse(substitute(ps_norm)), x='', y ='Proportional counts, log scale') + 
      scale_y_log10() + 
      scale_fill_manual(values=sgColorPalette)+
      theme_minimal() + facet_wrap(~rn, scales='free', ncol=3)+
      theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
  } else if (plot_type=='bar'){
    plot_table2 <- plot_table[, list(mean_ct=mean(value), sem=sd(value)/sqrt(.N)), by=c('Group', 'Within.study.sampling.date', 'rn')]
    ggplot(data=plot_table2, aes(x=Within.study.sampling.date, y =mean_ct, fill=Group)) + 
      geom_bar(stat='identity', position=position_dodge()) +
      geom_errorbar(aes(ymin=mean_ct-sem, ymax=mean_ct+sem), width=0.2, position=position_dodge(0.9))+ 
      labs(title =deparse(substitute(ps_norm)), x='', y ='Proportional counts, 0 to 1 scale') + 
      #scale_y_log10() + 
      scale_fill_manual(values=sgColorPalette)+
      theme_minimal() + facet_wrap(~rn, scales='free', ncol=3)+
      theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
  }
}

make_vis_plots_print <- function(ps_norm, grouping, tax, plot_type=c('box', 'bar'), plot_title){
  # ps should be a normalized (DESeq or CSS) phyloseq object
  # grouping should match the column name in the sample_data
  # tax is a taxonomical bin id (ASV) in the counts table to plot
  
  # subset phyloseq object to select ASV of interest
  ps_filt=prune_taxa(taxa_names(ps_norm) %in% tax, ps_norm)
  
  # get normalized counts
  plot_table<-data.table(otu_table(ps_filt), keep.rownames=TRUE)[rn %in% tax]
  # add very small value, min/100000 to 0
  plot_table <- melt(plot_table, id.vars='rn')
  plot_table$value <- plot_table$value+min(plot_table[value!=0]$value)/100000
  
  # add metadata
  groupDT=data.table(data.frame(sample_data(ps_filt)[, c(grouping, 'Within.study.sampling.date')]), keep.rownames=TRUE)
  setnames(groupDT, 'rn', 'variable')
  plot_table <- merge(plot_table, groupDT, by='variable', all.x=TRUE)
  
  # change variable to general name
  setnames(plot_table, grouping, 'Group')

  # boxplot
  if(plot_type=='box'){
    ggplot(data=plot_table, aes(x=Within.study.sampling.date, y = value, fill=Group)) + 
      geom_boxplot(outlier.color=NA) +
      geom_jitter(position=position_jitterdodge(0.2), cex=1.5, color="gray44") + 
      labs(title =deparse(substitute(ps_norm)), x='', y ='Proportional counts, log scale') + 
      scale_y_log10() + 
      scale_fill_manual(values=sgColorPalette)+
      theme_minimal() + facet_wrap(~rn, scales='free', ncol=3)+
      theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
  } else if (plot_type=='bar'){
    plot_table2 <- plot_table[, list(mean_ct=mean(value), sem=sd(value)/sqrt(.N)), by=c('Group', 'Within.study.sampling.date', 'rn')]
    ggplot(data=plot_table2, aes(x=Within.study.sampling.date, y =mean_ct, fill=Group)) + 
      geom_bar(stat='identity', position=position_dodge()) +
      geom_errorbar(aes(ymin=mean_ct-sem, ymax=mean_ct+sem), width=0.2, position=position_dodge(0.9))+ 
      labs(title =plot_title, x='', y ='Proportional counts, 0 to 1 scale') + 
      #scale_y_log10() + 
      scale_fill_manual(values=sgColorPalette)+
      theme_minimal() + facet_wrap(~rn, scales='free', ncol=3)+
      theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
  }
}

```

```{r visualization_sig}
######BOXPLOT of significant ones
# make significant taxa into one table so that all pvalues retained
significant_tax=NULL
significant_tax <- merge(data.table(deseq_res_P1[[2]], keep.rownames=TRUE)[, list(rn, deseq_P1_adjp=padj)],
                         data.table(deseq_res_P2[[2]], keep.rownames=TRUE)[, list(rn, deseq_P2_adjp=padj)],
                         by='rn', all=TRUE)
significant_tax <- merge(significant_tax,
                         data.table(deseq_res_P3[[2]], keep.rownames=TRUE)[, list(rn, deseq_P3_adjp=padj)],
                         by='rn', all=TRUE)
significant_tax <- merge(significant_tax,
                         data.table(bla[[2]], keep.rownames=TRUE)[, list(rn, deseq_timeseries_adjp=padj)],
                         by='rn', all=TRUE)
significant_tax <- merge(significant_tax,
                         data.table(zig_res_P1[[2]], keep.rownames=TRUE)[, list(rn, mtgseq_P1_adjp=adjPvalues)],
                         by='rn', all=TRUE)
significant_tax <- merge(significant_tax,
                         data.table(zig_res_P2[[2]], keep.rownames=TRUE)[, list(rn, mtgseq_P2_adjp=adjPvalues)],
                         by='rn', all=TRUE)
significant_tax <- merge(significant_tax,
                         data.table(zig_res_P3[[2]], keep.rownames=TRUE)[, list(rn, mtgseq_P3_adjp=adjPvalues)],
                         by='rn', all=TRUE)
significant_tax <- merge(significant_tax,
                         data.table(zig_res_all[[2]], keep.rownames=TRUE)[, list(rn, mtgseq_timeseries_adjp=adjPvalues)],
                         by='rn', all=TRUE)

# remove nothing
significant_tax <- significant_tax[rn!='nothing']

# write results
write.csv(significant_tax, file=paste0(output_data, 'Significant_res_deseq_q', deseq_cut, '_mtgseq_q', mtgseq_cut, '.csv'), row.names=FALSE)
          
datatable(significant_tax)

# also, find taxonomical annotations
# NOTE: single ASV may have multiple annotations due to tie hits
#Changing var all_tax_data to tax_table since I don't have this object since I don't have all_tax_data as a object,
datatable(tax_table(ps_not_norm_comp)[rownames(tax_table(ps_not_norm_comp)) %in% significant_tax$rn])
```

### DESeq results
```{r visualization_sig_deseq, fig.width=12}
## plot
# common by deseq
com_deseq_taxa=significant_tax[!is.na(deseq_P1_adjp) & !is.na(deseq_P2_adjp) & !is.na(deseq_P3_adjp)]

if(nrow(com_deseq_taxa)>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', com_deseq_taxa$rn, 'box'))
} else {
  print('no common DESeq significant taxa')
}
```

### DESeq timeseries results
```{r visualization_sig_deseq_ts, fig.width=12, fig.height=9}
# deseq timeseries
if(nrow(significant_tax[!is.na(deseq_timeseries_adjp)])>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(deseq_timeseries_adjp)]$rn, 'box'))
  # plot bar as well
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(deseq_timeseries_adjp)]$rn, 'bar'))
} else {
  print('no DESeq timeseries significant taxa')
}
```

### metagenomeSeq results
```{r visualization_sig_mtgseq, fig.width=12, fig.height=3}
# common by metagenomeseq
com_mtgseq_taxa=significant_tax[!is.na(mtgseq_P1_adjp) & !is.na(mtgseq_P2_adjp) & !is.na(mtgseq_P3_adjp)]

if(nrow(com_mtgseq_taxa)>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', com_mtgseq_taxa$rn, 'box'))
} else {
  print('no common metagenomeSeq significant taxa')
}
```
### Meta_genome timeseries results
```{r visualization_sig_mtg_ts, fig.width=12, fig.height=9}
# mtgseq timeseries
if(nrow(significant_tax[!is.na(mtgseq_timeseries_adjp)])>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[1:6], 'box'))
  # plot bar as well
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[1:6], 'bar'))
} else {
  print('no Mtgseq timeseries significant taxa')
}

if(nrow(significant_tax[!is.na(mtgseq_timeseries_adjp)])>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[7:12], 'box'))
  # plot bar as well
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[7:12], 'bar'))
} else {
  print('no Mtgseq timeseries significant taxa')
}

if(nrow(significant_tax[!is.na(mtgseq_timeseries_adjp)])>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[13:18], 'box'))
  # plot bar as well
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[13:18], 'bar'))
} else {
  print('no Mtgseq timeseries significant taxa')
}

if(nrow(significant_tax[!is.na(mtgseq_timeseries_adjp)])>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[19:24], 'box'))
  # plot bar as well
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[19:24], 'bar'))
} else {
  print('no Mtgseq timeseries significant taxa')
}

if(nrow(significant_tax[!is.na(mtgseq_timeseries_adjp)])>0){
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[25:27], 'box'))
  # plot bar as well
  print(make_vis_plots(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', significant_tax[!is.na(mtgseq_timeseries_adjp)]$rn[25:27], 'bar'))
} else {
  print('no Mtgseq timeseries significant taxa')
}
```

# CCA and visualization
Compare resulting amplicon data between and within sample types by canonical correlation analysis, regression profiling, and visualization (e.g. non-metric multi-dimensional scaling [NMDS], principle coordinates of analysis, principle component analysis).

## Constrained by status A or N
```{r cca_ind}
plotting_phenotype_consPcoA <- function(ps,title){
  fam_6<-names(table(sample_data(ps)$Family.group.ID)[table(sample_data(ps)$Family.group.ID) == 6])
  ps_6fam<-prune_samples(sample_data(ps)$Family.group.ID %in% fam_6,ps )
  ps_pcoa_ord <- ordinate(
    physeq = ps_6fam, 
    method = "CAP", 
    distance = "bray",
    formula = ~ phenotype
    )
  p<-plot_ordination(
    physeq = ps_6fam, 
    ordination = ps_pcoa_ord, 
    color = "phenotype", 
    axes = c(1,2),
    title= paste("Constrained PcoA",title,"ordinated by phenotype with all timepoints")
    ) + 
    geom_point( size = 2) +
    scale_color_manual(values=sgColorPalette)+
    theme_minimal()+
    theme(text = element_text(size =10), plot.title = element_text(size=10))
  
    #sum_pcoA_DesEq<-summary(ps_pcoa_ord)
    erie_bray_sum_pcoA <- phyloseq::distance(ps, method = "bray")
    sampledf <- data.frame(sample_data(ps))
    beta_di<-betadisper(erie_bray_sum_pcoA, sampledf$Family.group.ID)
to_return<-list()
to_return[[1]]<-p
to_return[[2]]<-beta_di
  return(to_return)
  }

#With Deseq
DeSeq_distance<-plotting_phenotype_consPcoA(ps_DeSeq_norm_pass_min_postDD_sup003, "Deseq")
# plot
DeSeq_distance[[1]]

#same with CSS 
CSS_distance<-plotting_phenotype_consPcoA(ps_CSS_norm_pass_min_postDD_sup003, "CSS")
# plot
CSS_distance[[1]]
```

## Constrained by family
```{r cca_family}
#plotting
#Now we have: 803 taxa and 559 samples

#Looking at the family fro the complete set of samples

#Keeping the same ordination but filtering to the families with only 6 point to help vizualize the plot

#Looking at NORMALIZATION
plotting_Fam_consPcoA <- function(ps,title){
  fam_6<-names(table(sample_data(ps)$Family.group.ID)[table(sample_data(ps)$Family.group.ID) == 6])
  ps_6fam<-prune_samples(sample_data(ps)$Family.group.ID %in% fam_6,ps )
  sample_data(ps_6fam)$Family.group.ID <- paste0('fam', as.character(sample_data(ps_6fam)$Family.group.ID))
  ps_pcoa_ord <- ordinate(
    physeq = ps_6fam, 
    method = "CAP", 
    distance = "bray",
    formula = ~ Family.group.ID
    )
  p<-plot_ordination(
    physeq = ps_6fam, 
    ordination = ps_pcoa_ord, 
    color = "Family.group.ID", 
    axes = c(1,2),
    title= paste("Constrained PcoA",title,"ordinated by families with all timepoints")
    ) + 
    geom_point( size = 2) +
    theme_minimal()+
    theme(text = element_text(size =10), plot.title = element_text(size=10), legend.position='none')
  
    #sum_pcoA_DesEq<-summary(ps_pcoa_ord)
    erie_bray_sum_pcoA <- phyloseq::distance(ps, method = "bray")
    sampledf <- data.frame(sample_data(ps))
    beta_di<-betadisper(erie_bray_sum_pcoA, sampledf$Family.group.ID)
to_return<-list()
to_return[[1]]<-p
to_return[[2]]<-beta_di
  return(to_return)
  }

#With Deseq
DeSeq_distance<-plotting_Fam_consPcoA(ps_DeSeq_norm_pass_min_postDD_sup003, "Deseq")
# plot
DeSeq_distance[[1]]

#same with CSS 
CSS_distance<-plotting_Fam_consPcoA(ps_CSS_norm_pass_min_postDD_sup003, "CSS")
# plot
CSS_distance[[1]]

#the distance in those plot?
#average_distance_to_median
#pdf(file=paste0(output_data, "Figures/Distance_DeSeq_CSS_", Sys.Date(), ".pdf"))
boxplot(DeSeq_distance[[2]]$distances,CSS_distance[[2]]$distances, names=c("DeSeq", "CSS"), 
        xlab = "Type of Normalization", ylab = "Distance on Component 1 & 2", main ="Intragroup distance for each family")
#dev.off()

```

# Diversity
Characterize and assess the diversity of each sample, and evaluate the extent of dissimilarity between the cohorts

## Alpha diversity
```{r alpha_diversity}
ER <- estimate_richness(ps_not_norm_comp, measures=c("Observed", "Chao1", "Shannon"))
ER <- cbind(ER, sample_data(ps_not_norm_comp)[row.names(ER), c("phenotype", "Family.group.ID", "Within.study.sampling.date")])
ER <- data.table(ER, keep.rownames = TRUE)
ER <- melt(ER, id.vars=c('rn', 'phenotype', "Family.group.ID", "Within.study.sampling.date"))

# plot
ggplot(data=ER[variable!='se.chao1'], aes(x=phenotype, y=value, fill=phenotype))+
  geom_boxplot(width=0.7, outlier.colour='white')+
  geom_jitter(size=1, position=position_jitter(width=0.1))+
  xlab('')+ylab('')+
  scale_fill_manual(values=sgColorPalette)+
  theme_minimal()+facet_wrap(~variable, scales='free')

# run t-test to check significance
ttest=NULL
for(alphad in c('Observed', 'Chao1', 'Shannon')){
  ttest_res=t.test(value ~ phenotype, data=ER[variable==alphad], var.equal=TRUE)
  ttest=rbindlist(list(ttest, data.table(alpha_index=alphad, pvalue=ttest_res$p.value)))
}

pander(ttest)
```

## Beta diversity
```{r beta_diversity}
#Let's do a PcoA #not much differences 
GP.ord <- ordinate(ps_DeSeq_norm_pass_min_postDD_sup003, "PCoA", "bray")
p2 = plot_ordination(ps_DeSeq_norm_pass_min_postDD_sup003, GP.ord, type="samples", color="phenotype") +
  geom_point( size = 1)+
  scale_color_manual(values=sgColorPalette)+
  theme_minimal()
p2
```


# PERMANOVA
non- parametric statistical approaches (ANOSIM, ADONIS, ANOVA, PERMANOVA, etc.) will be employed to determine the significance of noteworthy factors, such as digital phenotype, probiotic and/or antibiotic use

## PERMANOVA test
```{r permanova, results="asis"}
permanova <- function(physeq, factorName, ifnumeric, pmt=999){
  set.seed(1)
  braydist = phyloseq::distance(physeq, "bray")
  form <- as.formula(paste("braydist ~ ", c(factorName), sep = ""))
  metaDF=data.frame(sample_data(physeq)[, as.character(factorName)])
  # if numerical variable, make sure the class
  if(ifnumeric){
    metaDF[, factorName] <- as.numeric(metaDF[, factorName])
    factor_class='numeric'
  } else {
    factor_class='categorical'
  }
  perm <- adonis(form, permutations = pmt, metaDF)
  permDT=data.table(Variable=factorName, 
             FactorClass=factor_class,
             TotalN=perm$aov.tab['Total','Df']+1, 
             R2=perm$aov.tab[factorName, 'R2'], 
             pvalue=perm$aov.tab[factorName,'Pr(>F)'][1])
  return(permDT)
}
#betadispersion
#we keep only the cateory selected above as relevant 
tmp_metadat<-metadata_ok[,c(num_cat,fac_cat)]
#additionnal error to remove: not enough sample: 
tmp_metadat<-tmp_metadat[,-which(colnames(tmp_metadat) %in% c("Number.of.pet.reptiles","Number.of.pet.horses", "Pet.horse"))]
#additionnal error to remove: filled with only NA or one factor, cant do permutest on it due to adonis function requirements
col_levels<-sapply(tmp_metadat, levels)
col_levelscount<-sapply(col_levels, length)
tmp_metadat_1 <- tmp_metadat
#Since there are no numerics based on code below, will drop all that dont have 2 or more levels
#tmp_metadat[,which(sapply(tmp_metadat, class) == "numeric")]
tmp_metadat <- tmp_metadat[,which(col_levelscount >= 2)]

set.seed(1)
pval_factors_diper=c()
nb_samples_disper=c()
for (i in 1:length(tmp_metadat)){
  #cat (i,"\t")
  test_map<-tmp_metadat[!is.na(tmp_metadat[,i]) & tmp_metadat[,i] != "" ,]
  ps.tmp<-copy(ps_DeSeq_norm_pass_min_postDD_sup003)
  sample_data(ps.tmp) <- test_map
  df_metadata <- data.frame(sample_data(ps.tmp))
  df_metadata<-df_metadata[df_metadata[,colnames(test_map)[i]] != "",]
  # use prune_samples instead of subset_samples
  keepid=!is.na(get_variable(ps.tmp, colnames(test_map)[i])) &
    get_variable(ps.tmp, colnames(test_map)[i])!='' &
    get_variable(ps.tmp, colnames(test_map)[i])!='NA' 
  ps.tmp <- prune_samples(keepid, ps.tmp)
  #ps.tmp <- subset_samples(ps.tmp, colnames(test_map)[i] !="")
  tmp_nb_samples<-dim(otu_table(ps.tmp))[2]
  OTU_tables_bray <- phyloseq::distance(ps.tmp, method = "bray")
  beta <- betadisper(OTU_tables_bray, df_metadata[,colnames(test_map)[i]])
  tmp<-permutest(beta)
  tmp<-tmp$tab$`Pr(>F)`[1]
  pval_factors_diper<-c(pval_factors_diper,tmp)
  nb_samples_disper<-c(nb_samples_disper,tmp_nb_samples)}
#correct the p.value 
names(pval_factors_diper)<-colnames(tmp_metadat)
pval_factors_diper<-p.adjust(pval_factors_diper, method = "fdr")
to_remove_betadis<-names(pval_factors_diper)[pval_factors_diper<0.05]
  
# list of permanova variables
#meta_cat <- tibble(col_levelscount >= 2, colnames(tmp_metadat_1), sapply(tmp_metadat_1, class))
#rownames(meta_cat) <- colnames(tmp_metadat_1)
#colnames(meta_cat) <- c("permanova", "varname", "type")
#meta_cat$type <- gsub("factor", "Categorical", meta_cat$type)
#meta_cat$type <- gsub("numerical", "Continuous", meta_cat$type)

#meta_cat file listed phenotype as false for permanova, but I will add it back in)
meta_cat$permanova[which(meta_cat$varname == "phenotype")] <- "Categorical"
permanova_var=meta_cat[which(meta_cat$permanova!=FALSE),]

permanova_var$permanova[which(permanova_var$varname %in% c(dict_1_items, dict_2_items, "Stool.frequency"))] <- rep("Continuous", length(permanova_var$permanova[which(permanova_var$varname %in% c(dict_1_items, dict_2_items, "Stool.frequency"))]))

set.seed(1)
permanova_res=NULL
for(j in 1:nrow(permanova_var)){
    #print(factorName1)
    #pander(table(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)[, factorName1]))
  # variable name (as.characteradded)
  var_name=as.character(permanova_var$varname[j])
  # remove all NAs
  keepid=!is.na(get_variable(ps_DeSeq_norm_pass_min_postDD_sup003, var_name)) &
    get_variable(ps_DeSeq_norm_pass_min_postDD_sup003, var_name)!='NA' &
    get_variable(ps_DeSeq_norm_pass_min_postDD_sup003, var_name)!=''
  tmp_ps <- prune_samples(keepid, ps_DeSeq_norm_pass_min_postDD_sup003)
  
  # Check if there is more than 1 values (categories)
  if(uniqueN(sample_data(tmp_ps)[, var_name])>1){
    
    # if categorical
    if(permanova_var$permanova[j]=='Categorical'){
      # run permanova only if there are more than 1 groups
      p <- permanova(tmp_ps, factorName=var_name, ifnumeric=FALSE, pmt=999)
      permanova_res=rbindlist(list(permanova_res, p))
      rm(p)
    }
    # if continuous
    if(permanova_var$permanova[j]=='Continuous'){
      p <- permanova(tmp_ps, factorName=var_name, ifnumeric=TRUE, pmt=999)
      permanova_res=rbindlist(list(permanova_res, p))
      rm(p)
    }
  }
  rm(var_name)
}

# write
write.csv(permanova_res, file=paste0(output_data, 'PERMANOVA.csv'), row.names=FALSE)

# total number of variables tested
uniqueN(permanova_res$Variable)
# Factor class
pander(table(permanova_res$FactorClass))
# number of significant variables
uniqueN(permanova_res[pvalue<permanova_pcut]$Variable)



#and now removing the ones with betadispersion significant 
impacting_compo<-setdiff(permanova_res[pvalue<permanova_pcut]$Variable,  to_remove_betadis)

#and now the ones also significant between the two cohorts
impacting_compo<-impacting_compo[impacting_compo %in% c(names(all_chisquare), num_confounds_all)]
permanova_res<- permanova_res[permanova_res$Variable %in% impacting_compo,]

#removing LR predictions since those are essentially an indicator of phenotype and not confounding variables
permanova_res<-permanova_res[-which(permanova_res$Variable %in% c("LR10.probability.ASD..M3.", "LR5.probability.ASD..M3.", "LR6.probability.ASD..M3." , "LR10.prediction..M3.", "LR10.probability.not.ASD..M3.", "LR5.probability.not.ASD..M3." , "LR5.prediction..M3.", "LR6.prediction..M3." , "LR6.probability.not.ASD..M3.")),]

# sort
permanova_res <- permanova_res[order(R2, decreasing=TRUE)]
datatable(permanova_res)
write.csv(permanova_res, file=paste0(output_data, 'PERMANOV_betadis_imp_corhort.csv'), row.names=FALSE)

```

## Visualize permanova significant variables on PCoA
```{r plot_R2_value, fig.width=16, fig.height=40}
# function to plot PCoA, only for higher R2 value 
imp_factors<-permanova_res$Variable[permanova_res$R2 > 0.01]

imp<-list()
for (i in 1:length(imp_factors)){
if(anyNA(map[,imp_factors[i]]) == FALSE){
  imp[i] <- imp_factors[i]
}
}


impforpcoa<-unlist(imp)

#add<-paste(impforpcoa, collapse = " + ")

#copy and paste form variable below into formula for pcoa for convenience (not sure why it does not work as an input for formula, but copy/paste as text works)
#form<- as.formula(paste0("~ ", add))

ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa <- prune_samples(sample_names(ps_DeSeq_norm_pass_min_postDD_sup003)[which(is.na(ps_DeSeq_norm_pass_min_postDD_sup003@sam_data$Functional.bowel.finding) == FALSE)], ps_DeSeq_norm_pass_min_postDD_sup003)

ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa <- prune_samples(sample_names(ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa)[which(is.na(ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa@sam_data$Seafood..consumption.frequency.) == FALSE)], ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa)

ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa <- prune_samples(sample_names(ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa)[which(is.na(ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa@sam_data$Fruit..consumption.frequency...longitudinal.) == FALSE)], ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa)


#ordination formula .
ps_pcoa <- ordinate(
  physeq = ps_DeSeq_norm_pass_min_postDD_sup003_bowel_pcoa, 
  method = "CAP", 
  distance = "bray",
  #Did not include Toilet.cover and Meat/Seafood Longitudinal, Fruit..consumption.frequency...longitudinal. and LR10.prediction..M3. due to NA missing values which does not allow for ordination
  #formula = ~Age..months. + Age..years. + Stool.frequency + Vitamin.D..consumption.frequency.)
  formula = ~Age..months. + Functional.bowel.finding + Seafood..consumption.frequency. + Fruit..consumption.frequency...longitudinal.)
title_prep<-c(impforpcoa, "Functional.bowel.finding", "Seafood..consumption.frequency.", "Fruit..consumption.frequency...longitudinal.")

to_plot=list()
for (i in 1:length(title_prep)){
  to_plot[[i]] <- plot_ordination(
  physeq = ps_DeSeq_norm_pass_min_postDD_sup003, 
  ordination = ps_pcoa, 
  color = title_prep[i], 
  axes = c(1,2),
  title=title_prep[i]
) + 
  geom_point( size = 0.5) +
  theme(text = element_text(size =20), plot.title = element_text(size=15))
}
to_plot[[length(title_prep)+1]]<-plot_ordination(physeq = ps_DeSeq_norm_pass_min_postDD_sup003, ordination = ps_pcoa, type="taxa",title ="Taxa") + theme(text = element_text(size =15))
lay <- rbind(c(1),
             c(2),
             c(3),
             c(4),
             c(5),
             c(6))


#pdf(paste0(output_data,"confounding_factors.pdf",width=16,height=40))
grid.arrange(grobs = to_plot, layout_matrix = lay)
top_potential_confounds <- imp_factors
top_potential_confounds


  GI_problem_plo <- plot_ordination(
  physeq = ps_DeSeq_norm_pass_min_postDD_sup003, 
  ordination = ps_pcoa, 
  color = "Most.recent.GI.episode.symptoms..M3.", 
  axes = c(1,2),
  title=title_prep[i] + 
  geom_point( size = 0.5) +
  theme(text = element_text(size =20), plot.title = element_text(size=15)))


#dev.off()
```

```{r plot_imp_taxa}
#Let's have a look at the plot 
plot_ordination(physeq = ps_DeSeq_norm_pass_min_postDD_sup003, ordination = ps_pcoa, type="taxa",title ="Taxa") + theme(text = element_text(size =8))
#ok let's try to find the spcies that show some importance in this PCA
taxa.to.select<-vegan::scores(ps_pcoa)$species
#now plot it with no name for visibilty
rownames(taxa.to.select)<-c()
s.arrow(taxa.to.select) #the taxa that influence the most the plots are above 0.25
taxa.to.select.to.rem<-vegan::scores(ps_pcoa)$species[abs(vegan::scores(ps_pcoa)$species[,1])>0.1 | abs(vegan::scores(ps_pcoa)$species[,2])>0.1,]
#any overlap with the 5 important? 
rownames(bla[[2]]) %in% taxa.to.select.to.rem #NOPE!
```

## Comparison of Variance between Subjects and between Phenotypes
```{r variance}

#Comparing variance w/ avg difference in distance
tmpps <- prune_samples((ps_DeSeq_norm_pass_min_postDD_sup003_full@sam_data$Family.group.ID == "1"), ps_DeSeq_norm_pass_min_postDD_sup003_full)
tmppsA <- prune_samples(tmpps@sam_data$phenotype == "A", tmpps)
tmppsN <- prune_samples(tmpps@sam_data$phenotype == "N", tmpps)

A<-distance(tmppsA, "bray", type = "samples")
ave_distanceA=ave(c(A[1],A[2],A[3]))[1]

N<-distance(tmppsN, "bray", type = "samples")
ave_distanceN=ave(c(N[1],N[2],N[3]))[1]

tab<-tibble(ave_distanceA, ave_distanceN, tmpps@sam_data$Family.group.ID[1])
colnames(tab) <- c("AvgDistanceAut", "AvgDistanceNeu", "Family")


for(i in unique(ps_DeSeq_norm_pass_min_postDD_sup003_full@sam_data$Family.group.ID)){
tmpps <- prune_samples((ps_DeSeq_norm_pass_min_postDD_sup003@sam_data$Family.group.ID == i), ps_DeSeq_norm_pass_min_postDD_sup003_full)
tmppsA <- prune_samples(tmpps@sam_data$phenotype == "A", tmpps)
tmppsN <- prune_samples(tmpps@sam_data$phenotype == "N", tmpps)

A<-distance(tmppsA, "bray", type = "samples")
ave_distanceA=ave(c(A[1],A[2],A[3]))[1]

N<-distance(tmppsN, "bray", type = "samples")
ave_distanceN=ave(c(N[1],N[2],N[3]))[1]

tabtmp<-tibble(ave_distanceA, ave_distanceN, tmpps@sam_data$Family.group.ID[1])
colnames(tabtmp) <- c("AvgDistanceAut", "AvgDistanceNeu", "Family")

tab<-rbind(tab, tabtmp)
}


# run tests to check significance
taba<-tibble(tab$AvgDistanceAut, rep("A", length(tab$AvgDistanceAut)), tab$Family)
colnames(taba) <- c("AvgDistanceDiff_btwnTimepoints","phenotype", "Family" )

tabn<-tibble(tab$AvgDistanceNeu, rep("N", length(tab$AvgDistanceNeu)), tab$Family)
colnames(tabn) <- c("AvgDistanceDiff_btwnTimepoints","phenotype" , "Family")

finaltab2<-rbind(tabn, taba)

p <- ggplot(finaltab2, aes(x=phenotype, y=AvgDistanceDiff_btwnTimepoints)) + 
  geom_boxplot()
p + geom_jitter(shape=16, position=position_jitter(0.2))

# run tests to check significance
shapiro.test(finaltab2$AvgDistanceDiff_btwnTimepoints) #not normal we need a reanking test
wilcox.test(AvgDistanceDiff_btwnTimepoints ~ phenotype, data=finaltab2, var.equal=FALSE)

#not significant


#paired wilcoxon
tmpps <- prune_samples((ps_DeSeq_norm_pass_min_postDD_sup003_full@sam_data$Family.group.ID == "1"), ps_DeSeq_norm_pass_min_postDD_sup003_full)
tmppsA <- prune_samples(tmpps@sam_data$phenotype == "A", tmpps)
tmppsN <- prune_samples(tmpps@sam_data$phenotype == "N", tmpps)

A<-distance(tmppsA, "bray", type = "samples")
distanceA=c(A[1],A[2],A[3])

N<-distance(tmppsN, "bray", type = "samples")
distanceN=c(N[1],N[2],N[3])

tab<-tibble(distanceA, distanceN, rep(tmpps@sam_data$Family.group.ID[1], length(distanceA)), c("Timepoint 1-2", "Timepoint 1-3", "Timepoint 2-3"))
colnames(tab) <- c("DistanceAut", "DistanceNeu", "Family", "Timepoint")


for(i in unique(ps_DeSeq_norm_pass_min_postDD_sup003_full@sam_data$Family.group.ID)[c(-1)]){
tmpps <- prune_samples((ps_DeSeq_norm_pass_min_postDD_sup003_full@sam_data$Family.group.ID == i), ps_DeSeq_norm_pass_min_postDD_sup003_full)
tmppsA <- prune_samples(tmpps@sam_data$phenotype == "A", tmpps)
tmppsN <- prune_samples(tmpps@sam_data$phenotype == "N", tmpps)

A<-distance(tmppsA, "bray", type = "samples")
distanceA=c(A[1],A[2],A[3])

N<-distance(tmppsN, "bray", type = "samples")
distanceN=c(N[1],N[2],N[3])

tabtmp<-tibble(distanceA, distanceN, rep(tmpps@sam_data$Family.group.ID[1], length(distanceA)),  c("Timepoint 1-2", "Timepoint 1-3", "Timepoint 2-3"))
colnames(tabtmp) <- c("DistanceAut", "DistanceNeu", "Family", "Timepoint")

tab<-rbind(tab, tabtmp)
}
tab_w_tp <- tab

# run tests to check significance
taba<-tibble(tab$DistanceAut, rep("A", length(tab$DistanceAut)), tab$Family, tab$Timepoint)
colnames(taba) <- c("Distances_btwnTimepoints","phenotype", "Family" , "Timepoint")

tabn<-tibble(tab$DistanceNeu, rep("N", length(tab$DistanceNeu)), tab$Family, tab$Timepoint)
colnames(tabn) <- c("Distances_btwnTimepoints","phenotype", "Family" , "Timepoint")

finaltab3<-rbind(tabn, taba)

p <- ggplot(finaltab3, aes(x=phenotype, y=Distances_btwnTimepoints)) + 
  geom_boxplot()
p + geom_jitter(shape=16, position=position_jitter(0.2))

# run tests to check significance
wilcox.test(tab$DistanceAut, tab$DistanceNeu, paired = TRUE)


#still not significant




#Will do permutations


#with avgs
permutation_meandist_gen<-function(x){
  ptab <- x
  ptab$permu_label<- ptab$phenotype[shuffle(ptab$phenotype)]

  mean(ptab$AvgDistanceDiff_btwnTimepoints[which(ptab$permu_label == "A")]) -
  mean(ptab$AvgDistanceDiff_btwnTimepoints[which(ptab$permu_label == "N")])

}
permu_means<-replicate(1000, permutation_meandist_gen(finaltab2))

diff.means<-mean(finaltab2$AvgDistanceDiff_btwnTimepoints[which(finaltab2$phenotype == "A")]) -
  mean(finaltab2$AvgDistanceDiff_btwnTimepoints[which(finaltab2$phenotype == "N")])

sig <- sum(permu_means > diff.means)

hist(permu_means) 



# with raw values
permutation_meandist_gen<-function(x){
  ptab <- x
  ptab$permu_label<- ptab$phenotype[shuffle(ptab$phenotype)]

  mean(ptab$Distances_btwnTimepoints[which(ptab$permu_label == "A")]) -
  mean(ptab$Distances_btwnTimepoints[which(ptab$permu_label == "N")])

}
set.seed(1)
permu_means<-replicate(1000, permutation_meandist_gen(finaltab3))

diff.means<-mean(finaltab3$Distances_btwnTimepoints[which(finaltab3$phenotype == "A")]) -
  mean(finaltab3$Distances_btwnTimepoints[which(finaltab3$phenotype == "N")])

sig <- as.numeric(sum(permu_means >= diff.means))
pval<-sig/1000

pval

{hist(permu_means)
 abline(v = diff.means, col = "red")}

{plot(density(permu_means))
  abline(v = diff.means, col = "red")}


#organized by family

permutation_meandist_gen_by_fam<-function(x){
  ptab <- x
  tmp <- ptab[which(ptab$Family == ptab$Family[1]),]
  tmp$permu_label<- tmp$phenotype[shuffle(tmp$phenotype)]
  ptab_all <- tmp
  
  for (i in 2:length(unique(ptab$Family))){
  tmp <- ptab[which(ptab$Family == unique(ptab$Family)[i]),]
  tmp$permu_label<- tmp$phenotype[shuffle(tmp$phenotype)]
  ptab_all <- rbind(ptab_all, tmp)
  }
  

  mean(ptab_all$Distances_btwnTimepoints[which(ptab_all$permu_label == "A")]) -
  mean(ptab_all$Distances_btwnTimepoints[which(ptab_all$permu_label == "N")])

}
set.seed(1)
permu_means_by_fam<-replicate(1000, permutation_meandist_gen_by_fam(finaltab3))


sig <- as.numeric(sum(permu_means >= diff.means))
pval_by_fam<-sig/1000

pval_by_fam

{hist(permu_means_by_fam)
 abline(v = diff.means, col = "red")}

{plot(density(permu_means_by_fam))
  abline(v = diff.means, col = "red")}



# Now with difference between A and N at each time point, then taking mean


#Since they are in order by family and timepoint, I can subtract across

#view to make sure
#finaltab3[which(finaltab3$phenotype == "A"),]
#finaltab3[which(finaltab3$phenotype == "N"),]



# First just by timepoint
permutation_meandist_gen_by_fam_diff<-function(x){
  ptab <- x
  tmp <- ptab[which(ptab$Family == ptab$Family[1]),]
  tmp$permu_label<- tmp$phenotype[shuffle(tmp$phenotype)]
  ptab_all <- tmp
  
  for (i in 2:length(unique(ptab$Family))){
  tmp <- ptab[which(ptab$Family == unique(ptab$Family)[i]),]
  tmp$permu_label<- tmp$phenotype[shuffle(tmp$phenotype)]
  ptab_all <- rbind(ptab_all, tmp)
  }
  

  mean(ptab_all$Distances_btwnTimepoints[which(ptab_all$permu_label == "A")]-
  ptab_all$Distances_btwnTimepoints[which(ptab_all$permu_label == "N")])

}

fintab_1_2<-finaltab3[which(finaltab3$Timepoint == "Timepoint 1-2"),]
fintab_1_3<-finaltab3[which(finaltab3$Timepoint == "Timepoint 1-3"),]
fintab_2_3<-finaltab3[which(finaltab3$Timepoint == "Timepoint 2-3"),]

set.seed(1)
permu_means_by_fam12<-replicate(1000, permutation_meandist_gen_by_fam_diff(fintab_1_2))
mean_difference<-mean(fintab_1_2$Distances_btwnTimepoints[which(fintab_1_2$phenotype == "A")] -fintab_1_2$Distances_btwnTimepoints[which(fintab_1_2$phenotype == "N")])
sig <- as.numeric(sum(permu_means_by_fam12 >= mean_difference))
pval_by_fam12<-sig/1000

set.seed(1)
permu_means_by_fam13<-replicate(1000, permutation_meandist_gen_by_fam_diff(fintab_1_3))
mean_difference<-mean(fintab_1_3$Distances_btwnTimepoints[which(fintab_1_3$phenotype == "A")] -fintab_1_3$Distances_btwnTimepoints[which(fintab_1_3$phenotype == "N")])
sig <- as.numeric(sum(permu_means_by_fam13 >= mean_difference))
pval_by_fam13<-sig/1000


set.seed(1)
permu_means_by_fam23<-replicate(1000, permutation_meandist_gen_by_fam_diff(fintab_2_3))
mean_difference<-mean(fintab_2_3$Distances_btwnTimepoints[which(fintab_2_3$phenotype == "A")] -fintab_2_3$Distances_btwnTimepoints[which(fintab_2_3$phenotype == "N")])
sig <- as.numeric(sum(permu_means_by_fam23 >= mean_difference))
pval_by_fam23<-sig/1000


#All_together
permutation_meandist_gen_by_fam_diff_all<-function(x){
  ptab <- x
  tmp <- ptab[which(ptab$Family == ptab$Family[1]),]
  tmp_time <- tmp[which(tmp$Timepoint == unique(tmp$Timepoint)[1]),]
  tmp_time$permu_label<- tmp_time$phenotype[shuffle(tmp_time$phenotype)]
  tmp_time_all <- tmp_time
  
  for (b in 2:3) {
    tmp_time <- tmp[which(tmp$Timepoint == unique(tmp$Timepoint)[b]),]
    tmp_time$permu_label<- tmp_time$phenotype[shuffle(tmp_time$phenotype)]
    tmp_time_all <- rbind(tmp_time_all, tmp_time)
    
  }
  
  ptab_all <-tmp_time_all
  
  for (i in 2:length(unique(ptab$Family))){
  tmp <- ptab[which(ptab$Family == unique(ptab$Family)[i]),]
  tmp_time <- tmp[which(tmp$Timepoint == unique(tmp$Timepoint)[1]),]
  tmp_time$permu_label<- tmp_time$phenotype[shuffle(tmp_time$phenotype)]
  tmp_time_all <- tmp_time
  
    for (b in 2:3) {
    tmp_time <- tmp[which(tmp$Timepoint == unique(tmp$Timepoint)[b]),]
    tmp_time$permu_label<- tmp_time$phenotype[shuffle(tmp_time$phenotype)]
    tmp_time_all <- rbind(tmp_time_all, tmp_time)
    
    }
  
  ptab_all <- rbind(ptab_all, tmp_time_all)
  }
  

  mean(ptab_all$Distances_btwnTimepoints[which(ptab_all$permu_label == "A")]-
  ptab_all$Distances_btwnTimepoints[which(ptab_all$permu_label == "N")])

}

permu_means_by_fam_shuffled_while_maintaining_tp<-replicate(1000, permutation_meandist_gen_by_fam_diff(finaltab3))
mean_difference<-mean(finaltab3$Distances_btwnTimepoints[which(finaltab3$phenotype == "A")] -finaltab3$Distances_btwnTimepoints[which(finaltab3$phenotype == "N")])
sig <- as.numeric(sum(permu_means_by_fam_shuffled_while_maintaining_tp >= mean_difference))
pval_by_fam_all<-sig/1000

{plot(density(permu_means_by_fam_shuffled_while_maintaining_tp))
  abline(v = mean_difference, col = "red")}



```


## Without NA points

```{r plot_wo_na}
# function to plot PCoA without NA points
wo_na_pcoa <- function(ps, pvar, ord_res){
  # ord_res: ordinated object
  
  keepid=!is.na(get_variable(ps, pvar)) &
    get_variable(ps, pvar)!='NA' &
    get_variable(ps, pvar)!=''
  tmp_ps <- prune_samples(keepid, ps)
  
  # get subset counts and metadata together
  DF <- cbind(ord_res$vectors[row.names(sample_data(tmp_ps)), 1:2], sample_data(tmp_ps)[, pvar])
  setnames(DF, pvar, 'testvar')
  
  # get eigenvalues
  eig=(ord_res$values$Eigenvalues/sum(ord_res$values$Eigenvalues))[1:2]*100
  
  p <- ggplot(data=DF, aes(x=Axis.1, y=Axis.2, colour=testvar))+
    geom_point(size=2)+
    ggtitle(pvar)+
    xlab(paste0('Axis.1 [', format(eig[1], digits=3), '%]'))+
    ylab(paste0('Axis.2 [', format(eig[2], digits=3), '%]'))+
    theme_minimal()+
    theme(legend.title=element_blank(), legend.position="bottom")
    
  print(p)
}
```


### Looking into confounding variables
```{r confounds}

#Hard to find a confounding variable in impfactors that does not have a lot of NAs (no NAs required for DESEQ2) will put through metagenomeseq

#Function Updated with Altered formula for confound var
runDESeq_time_confound <- function(ps, dcut, confound){
  diagdds = phyloseq_to_deseq2(ps, as.formula(paste0("~ ", confound, "+ Within.study.sampling.date"))) 
  diagdds <- estimateSizeFactors(diagdds, type = "poscounts")
  diagdds <- DESeq(diagdds,fitType="parametric", betaPrior = FALSE) 
  #resultsNames(diagdds): to determine the constrast
  res = results(diagdds, contrast = c(confound, levels(map[,confound])[1], levels(map[,confound])[2]))
  res$padj[is.na(res$padj)] = 1
  sig <- res[res$padj < dcut,]
  if (dim(sig)[1] == 0) 
  {sigtab<- as.data.frame(1, row.names="nothing")
  colnames(sigtab) <- 'padj'}
  else 
  {
    sigtab <- data.frame(sig)
  }
  return(list(res, sigtab))
}

#Function Updated with Altered formula for confound var
run_metagenom_seq_confound<-function(ps,maxit, mcut, confound){
  p_metag<-phyloseq_to_metagenomeSeq(ps)
  #filtering at least 4 samples 
  p_metag= cumNorm(p_metag, p=0.75)
  normFactor =normFactors(p_metag)
  normFactor =log2(normFactor/median(normFactor) + 1)
  #mod = model.matrix(~ASDorNeuroT +PairASD+ normFactor)
  mod = model.matrix(as.formula(paste0("~ ", confound, "+ Within.study.sampling.date +normFactor")), data = pData(p_metag))
  settings =zigControl(maxit =maxit, verbose =FALSE)
  #settings =zigControl(tol = 1e-5, maxit = 30, verbose = TRUE, pvalMethod = 'bootstrap')
  fit =fitZig(obj = p_metag, mod = mod, useCSSoffset = FALSE, control = settings)
  #Note: changed fit$taxa to fit@taxa in light of error (probably from newer metagenomeseq ver.)
  res_fit<-MRtable(fit, number = length(fit@taxa))
  res_fit_nonfiltered <- copy(res_fit)
  res_fit<-res_fit[res_fit$adjPvalues<mcut,]
  #finally remove the ones that are not with enough samples
  #mean_sample<-mean(calculateEffectiveSamples(fit))
  #res_fit<-res_fit[res_fit$`counts in group 0` & res_fit$`counts in group 1` > mean_sample,]
  Min_effec_samp<-calculateEffectiveSamples(fit)
  Min_effec_samp<-Min_effec_samp[ names(Min_effec_samp)  %in% rownames(res_fit)] #####there is a bug here 
  #manually removing the ones with "NA"
  res_fit<-res_fit[grep("NA",rownames(res_fit), inv=T),]
  res_fit$Min_sample<-Min_effec_samp
  res_fit<-res_fit[res_fit$`+samples in group 0` >= Min_effec_samp & res_fit$`+samples in group 1` >= Min_effec_samp,]
  return(list(res_fit_nonfiltered, res_fit))
}

cat_confounds<-permanova_res$Variable[permanova_res$FactorClass == "categorical"]
num_confounds<-permanova_res$Variable[permanova_res$FactorClass == "numeric"]

#Remove var with more than 40 NAs
toomanyNAs<-list()
for (i in 1:length(cat_confounds)){
tmp<-is.na(filtered_ps003@sam_data[,cat_confounds[i]])
if (length(rownames(tmp)[which(tmp == TRUE)]) >=40) {
  toomanyNAs[i] <-cat_confounds[i]
}
}
cat_confounds<-cat_confounds[-which(cat_confounds %in% unlist(toomanyNAs))]

confound <- cat_confounds[1]
#some are listed as logical
write.csv(sample_data(filtered_ps003), "~/M3_Datasets/sam_data.csv")
map<-read.csv("~/M3_Datasets/sam_data.csv")
map[,confound] <- as.factor(map[,confound])
rownames(map) <- map$Biospecimen.Barcode
sample_data(filtered_ps003) <- map
filtered_ps003NAout<-prune_samples(!is.na(map[,confound]), filtered_ps003)
if(levels(map[,confound]) == 2){
deseqcon<-runDESeq_time_confound(filtered_ps003NAout, deseq_cut, confound = confound)
mtgcon<-run_metagenom_seq_confound(filtered_ps003NAout, 30, mtgseq_cut, confound = confound)
affected_taxa <-c(rownames(mtgcon[[2]]), row.names(deseqcon[2][[1]]) )
} else{
  mtgcon<-run_metagenom_seq_confound(filtered_ps003NAout, 30, mtgseq_cut, confound = confound)
  affected_taxa <-rownames(mtgcon[[2]])
}


#Run Deseq2 or Metagenomeseq on categorical confounds

for (i in 2:length(cat_confounds)) {
  confound <- cat_confounds[i]
  #some are listed as logical
  write.csv(sample_data(filtered_ps003), "sam_data.csv")
  map<-read.csv("sam_data.csv")
  map[,confound] <- as.factor(map[,confound])
  rownames(map) <- map$Biospecimen.Barcode
  sample_data(filtered_ps003) <- map
  filtered_ps003NAout<- prune_samples(!is.na(map[,confound]), filtered_ps003)
  if(length(levels(map[,confound])) == 2){
  deseqcon<-runDESeq_time_confound(filtered_ps003NAout, deseq_cut, confound = confound)
  mtgcon<-run_metagenom_seq_confound(filtered_ps003NAout, 30, mtgseq_cut, confound = confound)
  tmp <-c(rownames(mtgcon[[2]]), row.names(deseqcon[2][[1]]) )
  } else{
  mtgcon<-run_metagenom_seq_confound(filtered_ps003NAout, 30, mtgseq_cut, confound = confound)
  tmp <-rownames(mtgcon[[2]])
  }
  affected_taxa<-c(affected_taxa, tmp)
}

affected_taxa_cat<-unique(affected_taxa)




#Testing to see if numerical variables differ in means using wilcox or t-tests
tmp<-shapiro.test(map[,num_confounds[1]])
  if (tmp$p.value <= 0.05) {
    res<-wilcox.test(as.formula(paste(num_confounds[1], "~ phenotype")), data=map, var.equal = FALSE)
    tab<-tibble(num_confounds[1], res$p.value, "wilcox")
    colnames(tab) <- c("Var", "p.value", "Type")
    numerical_test_btwn_pheno <- tab
  }else{
    res<-t.test(as.formula(paste(num_confounds[1], "~ phenotype")), data=map)
    tab<-tibble(num_confounds[1], res$p.value, "t.test")
    colnames(tab) <- c("Var", "p.value", "Type")
    numerical_test_btwn_pheno <- tab
  }


for (i in num_confounds[c(-1)]) {
  tmp<-shapiro.test(map[,i])
  if (tmp$p.value <= 0.05) {
    res<-wilcox.test(as.formula(paste(i, "~ phenotype")), data=map, var.equal = FALSE)
    tab<-tibble(i, res$p.value, "wilcox")
    colnames(tab) <- c("Var", "p.value", "Type")
    numerical_test_btwn_pheno <-rbind(numerical_test_btwn_pheno, tab)
  }else{
    res<-t.test(as.formula(paste(i, "~ phenotype")), data=map)
    tab<-tibble(i, res$p.value, "t.test")
    colnames(tab) <- c("Var", "p.value", "Type")
    numerical_test_btwn_pheno <-rbind(numerical_test_btwn_pheno, tab)
  }
}


numerical_test_btwn_pheno$p.value<-p.adjust(numerical_test_btwn_pheno$p.value)
sig_numvar<-numerical_test_btwn_pheno[which(numerical_test_btwn_pheno$p.value <= 0.05),]

num_confounds2 <- sig_numvar$Var






#Spearman test between taxa and possible confounding variables that are ordinal
confound <- num_confounds2[1]  
otus<-as.data.frame(otu_table(ps_DeSeq_norm_pass_min_postDD_sup003))
otus <- t(otus)
otus <- as.data.frame(otus)

otus$confound <- map[,confound]
form <-as.formula(paste("~", colnames(otus)[1], "+", "confound"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$confound <- map[,confound]
    form <-as.formula(paste("~", colnames(otus)[i], "+", "confound"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)
spear_sig_asvs_p.val<-speartesttab$otu[spear_p.val < 0.05]

for (x in 2:length(num_confounds2)){
  confound <- num_confounds2[x]  
  otus<-as.data.frame(otu_table(ps_DeSeq_norm_pass_min_postDD_sup003))
  otus <- t(otus)
  otus <- as.data.frame(otus)

  otus$confound <- map[,confound]
  form <-as.formula(paste("~", colnames(otus)[1], "+", "confound"))
  tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

  speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
  colnames(speartesttab) <- c("otu", "p_val")

  for (i in 2:(length(colnames(otus))-1)){
    otus$confound <- map[,confound]
    form <-as.formula(paste("~", colnames(otus)[i], "+", "confound"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

  spear_p.val<-p.adjust(speartesttab$p_val)
  spear_sig_asvs_p.val<-c(spear_sig_asvs_p.val, speartesttab$otu[spear_p.val < 0.05])

}

#generate full list of significant ones with confounds
affected_num_list <- unique(spear_sig_asvs_p.val)

full_confound_asv_list <-c(affected_num_list, affected_taxa_cat)

full_confound_asv_list<-unique(full_confound_asv_list)


#Filter them out from main list and save
full_sigtab_esv_confoundfiltered<-fullsigtab_esv[-which(rownames(fullsigtab_esv) %in% full_confound_asv_list), ]

write.csv(full_sigtab_esv_confoundfiltered, "Full_sig_asvs_w_confounding_var_asvs_filtered_out.csv")

confounding_var_list <-c(num_confounds2, cat_confounds)
saveRDS(confounding_var_list, "confounding_var_list.rds")


full_sigtab_esv_confoundfiltered.print <- full_sigtab_esv_confoundfiltered
rownames(full_sigtab_esv_confoundfiltered.print) <- NULL
full_sigtab_esv_confoundfiltered.print$ASV <- NULL

full_sigtab_esv_confoundfiltered.print

full_sigtab_esv_confoundfiltered$Genusp <- gsub("g__", "", full_sigtab_esv_confoundfiltered$Genus)

full_sigtab_esv_confoundfiltered$Classp <- gsub("c__", "", full_sigtab_esv_confoundfiltered$Class)


# plotting consistently enriched/depleted ones 
print(make_vis_plots_print(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', full_sigtab_esv_confoundfiltered$ASV[1], 'bar', full_sigtab_esv_confoundfiltered$Genusp[1]))

print(make_vis_plots_print(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', full_sigtab_esv_confoundfiltered$ASV[6], 'bar', full_sigtab_esv_confoundfiltered$Classp[6]))

print(make_vis_plots_print(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', full_sigtab_esv_confoundfiltered$ASV[7], 'bar', full_sigtab_esv_confoundfiltered$Genusp[7]))

print(make_vis_plots_print(ps_TSS_norm_pass_min_postDD_sup003, 'phenotype', full_sigtab_esv_confoundfiltered$ASV[8], 'bar', full_sigtab_esv_confoundfiltered$Genusp[8]))




```

### Digital phenotype
```{r pcoa_dig_phenotype, fig.height=5}
sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)$Mobile.Autism.Risk.Assessment.Score <- as.numeric(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)$Mobile.Autism.Risk.Assessment.Score)


#Spearman test for taxa correlated with MARA with Deseq
ps_DeSeq_norm_pass_min_postDD_sup003_A <- subset_samples(ps_DeSeq_norm_pass_min_postDD_sup003, phenotype == "A")

otus<-as.data.frame(otu_table(ps_DeSeq_norm_pass_min_postDD_sup003_A))
otus <- t(otus)
otus <- as.data.frame(otus)

mapa <- map[which(map$phenotype == "A"),]
otus$MARA <- mapa[,"Mobile.Autism.Risk.Assessment.Score"]
form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- mapa[,"Mobile.Autism.Risk.Assessment.Score"]
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_des_all_MARA<-speartesttab$otu[spear_p.val < 0.05]



#Spearman test for taxa correlated with MARA with CSS
ps_CSS_norm_pass_min_postDD_sup003_A <- subset_samples(ps_CSS_norm_pass_min_postDD_sup003, phenotype == "A")

otus<-as.data.frame(otu_table(ps_CSS_norm_pass_min_postDD_sup003_A))
otus <- t(otus)
otus <- as.data.frame(otus)

mapa <- map[which(map$phenotype == "A"),]
otus$MARA <- mapa[,"Mobile.Autism.Risk.Assessment.Score"]

form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- mapa[,"Mobile.Autism.Risk.Assessment.Score"]
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_des_all_MARA_CSS<-speartesttab$otu[spear_p.val < 0.05]

#Same asvs, but not found in fullsigtab_esv



#Trying by timepoint

#Timepoint 1

P1_des<-prune_samples(rownames(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003_A))[sample_data(ps_DeSeq_norm_pass_min_postDD_sup003_A)$Within.study.sampling.date == "Timepoint 1"], ps_DeSeq_norm_pass_min_postDD_sup003_A)


otus<-as.data.frame(otu_table(P1_des))
otus <- t(otus)
otus <- as.data.frame(otus)

otus$MARA <- P1_des@sam_data$Mobile.Autism.Risk.Assessment.Score
form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- P1_des@sam_data$Mobile.Autism.Risk.Assessment.Score
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_des_1_MARA<-speartesttab$otu[spear_p.val < 0.05]



#Timepoint 2

P2_des<-prune_samples(rownames(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003_A))[sample_data(ps_DeSeq_norm_pass_min_postDD_sup003_A)$Within.study.sampling.date == "Timepoint 2"], ps_DeSeq_norm_pass_min_postDD_sup003_A)



otus<-as.data.frame(otu_table(P2_des))
otus <- t(otus)
otus <- as.data.frame(otus)

otus$MARA <- P2_des@sam_data$Mobile.Autism.Risk.Assessment.Score
form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- P2_des@sam_data$Mobile.Autism.Risk.Assessment.Score
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_des_2_MARA<-speartesttab$otu[spear_p.val < 0.05]


#Timepoint 3

P3_des<-prune_samples(rownames(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003_A))[sample_data(ps_DeSeq_norm_pass_min_postDD_sup003_A)$Within.study.sampling.date == "Timepoint 3"], ps_DeSeq_norm_pass_min_postDD_sup003_A)

otus<-as.data.frame(otu_table(P3_des))
otus <- t(otus)
otus <- as.data.frame(otus)

otus$MARA <- P3_des@sam_data$Mobile.Autism.Risk.Assessment.Score
form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- P3_des@sam_data$Mobile.Autism.Risk.Assessment.Score
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_des_3_MARA<-speartesttab$otu[spear_p.val < 0.05]


#Trying by timepoint with CSS

#Timepoint 1

P1_CSS<-prune_samples(rownames(sample_data(ps_CSS_norm_pass_min_postDD_sup003_A))[sample_data(ps_CSS_norm_pass_min_postDD_sup003_A)$Within.study.sampling.date == "Timepoint 1"], ps_CSS_norm_pass_min_postDD_sup003_A)


otus<-as.data.frame(otu_table(P1_CSS))
otus <- t(otus)
otus <- as.data.frame(otus)

otus$MARA <- P1_CSS@sam_data$Mobile.Autism.Risk.Assessment.Score
form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- P1_CSS@sam_data$Mobile.Autism.Risk.Assessment.Score
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_CSS_1_MARA<-speartesttab$otu[spear_p.val < 0.05]



#Timepoint 2

P2_CSS<-prune_samples(rownames(sample_data(ps_CSS_norm_pass_min_postDD_sup003_A))[sample_data(ps_CSS_norm_pass_min_postDD_sup003_A)$Within.study.sampling.date == "Timepoint 2"], ps_CSS_norm_pass_min_postDD_sup003_A)



otus<-as.data.frame(otu_table(P2_CSS))
otus <- t(otus)
otus <- as.data.frame(otus)

otus$MARA <- P2_CSS@sam_data$Mobile.Autism.Risk.Assessment.Score
form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- P2_des@sam_data$Mobile.Autism.Risk.Assessment.Score
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_CSS_2_MARA<-speartesttab$otu[spear_p.val < 0.05]


#Timepoint 3

P3_CSS<-prune_samples(rownames(sample_data(ps_CSS_norm_pass_min_postDD_sup003_A))[sample_data(ps_CSS_norm_pass_min_postDD_sup003_A)$Within.study.sampling.date == "Timepoint 1"], ps_CSS_norm_pass_min_postDD_sup003_A)

otus<-as.data.frame(otu_table(P3_CSS))
otus <- t(otus)
otus <- as.data.frame(otus)

otus$MARA <- P3_CSS@sam_data$Mobile.Autism.Risk.Assessment.Score
form <-as.formula(paste("~", colnames(otus)[1], "+", "MARA"))
tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

speartesttab <- tibble(colnames(otus)[1], tmp$p.value)
colnames(speartesttab) <- c("otu", "p_val")

for (i in 2:(length(colnames(otus))-1)){
    otus$MARA <- P3_des@sam_data$Mobile.Autism.Risk.Assessment.Score
    form <-as.formula(paste("~", colnames(otus)[i], "+", "MARA"))
    tmp<-cor.test(formula = form, data = otus, method = "spearman",
         continuity = FALSE,
         conf.level = 0.95, exact = FALSE)

    tab <- tibble(colnames(otus)[i], tmp$p.value)
    colnames(tab) <- c("otu", "p_val")
    speartesttab <-rbind(speartesttab, tab)
}

spear_p.val<-p.adjust(speartesttab$p_val)

spear_sig_asvs_p.val_CSS_3_MARA<-speartesttab$otu[spear_p.val < 0.05]







wo_na_pcoa(ps_DeSeq_norm_pass_min_postDD_sup003, 'Mobile.Autism.Risk.Assessment.Score', GP.ord)
```

### Probiotics
```{r pcoa_probiotics, fig.height=5}
wo_na_pcoa(ps_DeSeq_norm_pass_min_postDD_sup003, 'Probiotic..consumption.frequency.', GP.ord)
```

### Antibiotics
```{r pcoa_antibiotics, fig.height=5}
# Anti.infective
wo_na_pcoa(ps_DeSeq_norm_pass_min_postDD_sup003, 'Anti.infective', GP.ord)

# Minimum.time.since.antibiotics
sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)$Minimum.time.since.antibiotics <- as.numeric(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)$Minimum.time.since.antibiotics)
wo_na_pcoa(ps_DeSeq_norm_pass_min_postDD_sup003, 'Minimum.time.since.antibiotics', GP.ord)

```

### All other passed R2 cutoff and significant
```{r pcoa_permanova_sig, fig.height=5}

for(pvar in permanova_res[R2>permanova_cut & pvalue<permanova_pcut]$Variable){
wo_na_pcoa(ps_DeSeq_norm_pass_min_postDD_sup003, pvar, GP.ord)
}

```

### Random Forest

```{r randomforest}

#Random Forest main function
rand_forest <- function(pred_sequences, ps){ 

  
  ps <-prune_taxa(pred_sequences, ps )
  phen <- sample_data(ps)$phenotype
  family <- unique(sample_data(ps)$Family.group.ID)
  fam_id <- sample_data(ps)$Family.group.ID
  
  data<-t(otu_table(ps))
  data <- data.frame(phen, data, fam_id)

  set.seed(103)
  folds_by_family<-groupKFold(fam_id, 10)
  
  validate <- data[-folds_by_family[[1]],]
  training <- data[folds_by_family[[1]],]


  validate$fam_id <- NULL
  training$fam_id <- NULL
  
  set.seed(1)
  control <- trainControl(method='repeatedcv', 
                        number=3, 
                        repeats=3)

  tunegrid <- expand.grid(.mtry=c(3:20)) #mtry is the depth of each decision tree
  rf <- train(phen ~., 
            data= training, 
            method='rf', 
            metric='Accuracy', 
            tuneGrid=tunegrid, 
            trControl=control)


  mtry_best = as.numeric(rf$bestTune)

  set.seed(1)
  AR.classify <- randomForest(phen~., data = training, ntree = 128, mtry = mtry_best, importance = TRUE)
  rf<-AR.classify
  OOB.votes <- predict(rf,validate[,-1],type="prob");
  OOBpred_votes <- OOB.votes

  pred_votes <- OOBpred_votes[,2]


  for(i in 2:10){
  
  
    validate <- data[-folds_by_family[[i]],]
    training <- data[folds_by_family[[i]],]

    validate$fam_id <- NULL
    training$fam_id <- NULL
  
  
    set.seed(1)
    control <- trainControl(method='repeatedcv', 
                        number=3, 
                        repeats=3)

    tunegrid <- expand.grid(.mtry=c(3:20)) #mtry is the depth of each decision tree
    rf <- train(phen ~., 
            data= training, 
            method='rf', 
            metric='Accuracy', 
            tuneGrid=tunegrid, 
            trControl=control)
  
    mtry_best = as.numeric(rf$bestTune)
  
    set.seed(1)
    AR.classify <- randomForest(phen~., data = training, ntree = 128, mty = mtry_best, importance = TRUE)
    rf<-AR.classify
    OOB.votes <- predict (rf,validate[,-1],type="prob");
  
    OOB.votes
    pred_votes<-append(pred_votes, OOB.votes[,2])
  
  
  }
  a<-tibble(names(pred_votes), pred_votes)
  b <- a[order(a$`names(pred_votes)`), ]
  data<- data[order(rownames(data)),]


  pred.obj <- prediction(b$pred_votes,data$phen);
  perf_AUC=performance(pred.obj,"auc") #Calculate the AUC value
  AUCagp1=perf_AUC@y.values[[1]]
  AUCagp1
  RP.perf <- performance(pred.obj, "prec","rec");
  ROC.perfAGP <- performance(pred.obj, "tpr","fpr");
  outputlist <-list()
  outputlist[1] <- AUCagp1
  outputlist[2] <- ROC.perfAGP
  return(outputlist)

}

#For 24 (set of asvs with confounds filtered out)
ROC.perf_24 <-rand_forest(pred_sequences = rownames(full_sigtab_esv_confoundfiltered), ps = ps_DeSeq_norm_pass_min_postDD_sup003)

#For 70 (unfiltered results)
ROC.perf_70 <-rand_forest(rownames(fullsigtab_esv), ps_DeSeq_norm_pass_min_postDD_sup003)

#For 11 (ASVs from the 24 who are consistantly enriched/depleted over timepoints)
ROC.perf_11_all <-rand_forest(pred_sequences = full_sigtab_esv_confoundfiltered_consist$ASV, ps = ps_DeSeq_norm_pass_min_postDD_sup003)



#Combine AUCs
AUC_all <- c(ROC.perf_24[1], ROC.perf_70[1])#, ROC.perf_11_all[1])

#Plot
{plot(ROC.perf_24[[2]], main = "10-Cross Validation for ASD Classification", col = rainbow(8)[1])
plot(ROC.perf_70[[2]], main = "10-cross validation", col = rainbow(8)[2], add = TRUE)
#plot(ROC.perf_11_all[[2]], main = "10-cross validation", col = rainbow(8)[3], add = TRUE)
lines(c(0,1), c(0,1), col = "black")
legend(title = "AUC value", .81, .33, legend=round(as.numeric(AUC_all), digits = 4),
       col=colors,cex=1.0)

predictors_name <- c("24 Diff. Abundant ASVs", "70 Diff. Abundant ASVs") #, "Selected 11 ASVs")
legend(title = "Predictors",.375, .33, legend=predictors_name,
       col=c(rainbow(8)[c(1,2,3,5,7,6,8)], hcl.colors(1, palette = "viridis")),cex=1.0, lty=1:7)
}




rand_forest_time <- function(pred_sequences, ps){ 
  
  ps <-prune_taxa(pred_sequences, ps )
  phen <- sample_data(ps)$phenotype
  data<-t(otu_table(ps))
  data <- data.frame(phen, data)
  set.seed(103)
  folds<-createFolds(data$phen, k=10)
  validate <- data[folds[[1]],]
  training <- data[-folds[[1]],]
  set.seed(1)
  control <- trainControl(method='repeatedcv', 
                        number=3, 
                        repeats=3)
  tunegrid <- expand.grid(.mtry=c(3:20)) #mtry is the depth of each decision tree
  rf <- train(phen ~., 
            data= training, 
            method='rf', 
            metric='Accuracy', 
            tuneGrid=tunegrid, 
            trControl=control)
  mtry_best = as.numeric(rf$bestTune)
  set.seed(1)
  AR.classify <- randomForest(phen~., data = training, ntree = 128, mtry = mtry_best, importance = TRUE)
  rf<-AR.classify
  OOB.votes <- predict(rf,validate[,-1],type="prob");
  OOBpred_votes <- OOB.votes
  pred_votes <- OOBpred_votes[,2]
  for(i in 2:10){
  
  
    validate <- data[folds[[i]],]
    training <- data[-folds[[i]],]
  
  
    set.seed(1)
    control <- trainControl(method='repeatedcv', 
                        number=3, 
                        repeats=3)
    tunegrid <- expand.grid(.mtry=c(3:20)) #mtry is the depth of each decision tree
    rf <- train(phen ~., 
            data= training, 
            method='rf', 
            metric='Accuracy', 
            tuneGrid=tunegrid, 
            trControl=control)
  
    mtry_best = as.numeric(rf$bestTune)
  
    set.seed(1)
    AR.classify <- randomForest(phen~., data = training, ntree = 128, mty = mtry_best, importance = TRUE)
    rf<-AR.classify
    OOB.votes <- predict (rf,validate[,-1],type="prob");
  
    OOB.votes
    pred_votes<-append(pred_votes, OOB.votes[,2])
  
  
  }
  a<-tibble(names(pred_votes), pred_votes)
  b <- a[order(a$`names(pred_votes)`), ]
  data<- data[order(rownames(data)),]
  pred.obj <- prediction(b$pred_votes,data$phen);
  perf_AUC=performance(pred.obj,"auc") #Calculate the AUC value
  AUCagp1=perf_AUC@y.values[[1]]
  AUCagp1
  RP.perf <- performance(pred.obj, "prec","rec");
  ROC.perfAGP <- performance(pred.obj, "tpr","fpr");
  outputlist <-list()
  outputlist[1] <- AUCagp1
  outputlist[2] <- ROC.perfAGP
  return(outputlist)
}
#Per timepoints

P1_des<-prune_samples(rownames(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003))[sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)$Within.study.sampling.date == "Timepoint 1"], ps_DeSeq_norm_pass_min_postDD_sup003)
P2_des<-prune_samples(rownames(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003))[sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)$Within.study.sampling.date == "Timepoint 2"], ps_DeSeq_norm_pass_min_postDD_sup003)
P3_des<-prune_samples(rownames(sample_data(ps_DeSeq_norm_pass_min_postDD_sup003))[sample_data(ps_DeSeq_norm_pass_min_postDD_sup003)$Within.study.sampling.date == "Timepoint 3"], ps_DeSeq_norm_pass_min_postDD_sup003)


ROC.perf_24_P1 <-rand_forest(pred_sequences = rownames(full_sigtab_esv_confoundfiltered), ps = P1_des)
ROC.perf_24_P2 <-rand_forest(pred_sequences = rownames(full_sigtab_esv_confoundfiltered), ps = P2_des)
ROC.perf_24_P3 <-rand_forest(pred_sequences = rownames(full_sigtab_esv_confoundfiltered), ps = P3_des)
AUC_all <- c(ROC.perf_24_P1[1], ROC.perf_24_P2[1], ROC.perf_24_P3[1])


{plot(ROC.perf_24_P1[[2]], main = "10-cross validation for 24 Sequences by Timepoint", col = rainbow(8)[1])
plot(ROC.perf_24_P2[[2]], main = "10-cross validation", col = rainbow(8)[2], add = TRUE)
plot(ROC.perf_24_P2[[2]], main = "10-cross validation", col = rainbow(8)[3], add = TRUE)
lines(c(0,1), c(0,1), col = "black")
legend(title = "AUC value", .8, .33, legend=round(as.numeric(AUC_all), digits = 4),
       col=colors,cex=1.0)

predictors_name <- c("24 Diff. Abundant ASVs P1", "24 Diff. Abundant ASVs P2","24 Diff. Abundant ASVs P3")
legend(title = "Predictors",.375, .33, legend=predictors_name,
       col=c(rainbow(8)[c(1,2,3,5,7,6,8)], hcl.colors(1, palette = "viridis")),cex=1.0, lty=1:7)
}


ROC.perf_70_P1 <-rand_forest(pred_sequences = rownames(fullsigtab_esv), ps = P1_des)
ROC.perf_70_P2 <-rand_forest(pred_sequences = rownames(fullsigtab_esv), ps = P2_des)
ROC.perf_70_P3 <-rand_forest(pred_sequences = rownames(fullsigtab_esv), ps = P3_des)
AUC_all <- c(ROC.perf_70_P1[1], ROC.perf_70_P2[1], ROC.perf_70_P3[1])


{plot(ROC.perf_70_P1[[2]], main = "10-cross validation for 70 Sequences by Timepoint", col = rainbow(8)[1])
plot(ROC.perf_70_P2[[2]], main = "10-cross validation", col = rainbow(8)[2], add = TRUE)
plot(ROC.perf_70_P2[[2]], main = "10-cross validation", col = rainbow(8)[3], add = TRUE)
lines(c(0,1), c(0,1), col = "black")
legend(title = "AUC value", .8, .33, legend=round(as.numeric(AUC_all), digits = 4),
       col=colors,cex=1.0)

predictors_name <- c("70 Diff. Abundant ASVs P1", "70 Diff. Abundant ASVs P2","70 Diff. Abundant ASVs P3")
legend(title = "Predictors",.375, .33, legend=predictors_name,
       col=c(rainbow(8)[c(1,2,3,5,7,6,8)], hcl.colors(1, palette = "viridis")),cex=1.0, lty=1:7)
}





```

### Heat Map
```{r heat map}
# Heatmap EXPERIMENTAL STAGE
heatmap <- ps_TSS_norm_pass_min_postDD_sup003@otu_table[fullsigtab_esv$ASV,]
heatlabel <-gsub("g__","",fullsigtab_esv$Genus)
for (x in 1:70){
  if (heatlabel[x] == "unclassified"){
    heatlabel[x]<-paste(gsub("c__", "", fullsigtab_esv$Class[x]), heatlabel[x], sep = "_")
  }
  heatlabel[x] <- paste(heatlabel[x], x, sep = ".")
}

rownames(heatmap) <- heatlabel
heatmap<-as.data.frame(heatmap)
heatmap <- t(heatmap)
heatmap<-as.data.frame(heatmap)
heatmap$Sample <- rownames(heatmap)
heatmap$Phenotype <- ps_TSS_norm_pass_min_postDD_sup003@sam_data$phenotype
heatmap_below0.5 <- heatmap
maxes<-list()
for (x in c(1:70)){
   maxes[x]<-max(heatmap_below0.5[,x])
}
heatmap_below0.5 <- heatmap[,-which(maxes >= 0.002)]

heatmelt <-melt(heatmap_below0.5, value.name = "Abundance")
ggplot(heatmelt, aes(Sample, variable, fill= Abundance)) + 
  geom_tile() + 
  facet_grid(~ Phenotype,switch = "x", scales = "free_x", space = "free_x") +
  scale_fill_gradient(name = "Abundance",
                      low = "#FFFFFF",
                      high = "#012345")

##24

heatmap <- ps_DeSeq_norm_pass_min_postDD_sup003@otu_table[full_sigtab_esv_confoundfiltered$ASV,]
heatlabel <-gsub("g__","",full_sigtab_esv_confoundfiltered$Genus)
for (x in 1:24){
  if (heatlabel[x] == "unclassified"){
    heatlabel[x]<-paste(gsub("c__", "", full_sigtab_esv_confoundfiltered$Class[x]), heatlabel[x], sep = "_")
  }
  heatlabel[x] <- paste(heatlabel[x], x, sep = ".")
}

rownames(heatmap) <- heatlabel
heatmap<-as.data.frame(heatmap)
heatmap <- t(heatmap)
heatmap<-as.data.frame(heatmap)
heatmap$Sample <- rownames(heatmap)
heatmap$Phenotype <- ps_DeSeq_norm_pass_min_postDD_sup003@sam_data$phenotype

heatmelt <-melt(heatmap, value.name = "Abundance")
ggplot(heatmelt, aes(Sample, variable, fill= Abundance)) + 
  geom_tile() +
  facet_grid(~ Phenotype,switch = "x", scales = "free_x", space = "free_x") +
  scale_fill_gradient(name = "Abundance",
                      low = "#FFFFFF",
                      high = "#012345")



```


# Session information

```{r sessionInfo}
sessionInfo()
```


